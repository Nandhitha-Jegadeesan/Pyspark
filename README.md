# Pyspark
This repository contains my hands-on practice scripts and learning notes related to PySpark, specifically for data processing and transformation using Spark DataFrames in Databricks environment.

Based on your uploaded Databricks notebook file titled **"Practise"**, here's a clean and concise `README.md` content you can use for your GitHub repository:

## ðŸ§  About This Notebook

This notebook includes exercises on:

* Reading and writing data using Spark
* Basic transformations using `withColumn`, `selectExpr`, `expr`, etc.
* Filtering and aggregations
* String manipulations and column operations
* Joins and self joins
* Window functions like `row_number()`, `rank()`, and `dense_rank()`
* Working with nulls and data cleaning
* Writing data to various formats (CSV, Parquet)

## ðŸ’¡ Technologies Used

* PySpark (Apache Spark with Python)
* Databricks (Community Edition)
* CSV data files

## ðŸ§ª Key Concepts Practiced

* SparkSession creation and configuration
* DataFrame creation and inspection
* Column-wise operations with expressions
* Using Window functions for advanced transformations
* Data filtering using SQL-style and DSL syntax
* Writing transformed data to files

## ðŸš€ Getting Started

To run this notebook:

1. Create a Databricks Community Edition account (if you donâ€™t already have one).
2. Import the `Practise.html` notebook into your workspace.
3. Attach it to a running cluster and execute the cells step-by-step.


**Nandhitha Sree Jegadeesan**
Big Data & PySpark Learner
[GitHub Profile](https://github.com/Nandhitha-Jegadeesan)


